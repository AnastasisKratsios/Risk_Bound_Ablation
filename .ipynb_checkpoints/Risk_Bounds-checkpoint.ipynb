{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620e92ab",
   "metadata": {},
   "source": [
    "# Plot Concentration Bound\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40067e9",
   "metadata": {},
   "source": [
    "### N\n",
    "Once the number of training samples, $N$, is set then, we consider two scenarios:\n",
    "- if ``N_is_Small==True``: $N$ ranges from $\\lfloor N/10 \\rfloor$ to $N$\n",
    "- if ``N_is_Small==True``: $N$ ranges from $N^3$ to $N^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ccfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1*(10**3)\n",
    "N_is_Small = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19c9b7",
   "metadata": {},
   "source": [
    "## Geometry of $\\mathcal{X}\\times \\mathcal{Y}$: \n",
    "### Set the Rate at Which Embedding Distorts Geometry in As a Function of Euclidean Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d926644",
   "metadata": {},
   "source": [
    "We consider four possible cases (``distortion_type``):\n",
    "1. Worst-case (expander graph-type geometries): see paper and results of Bourgain.\n",
    "2. Latent Full Binary-Tree Structure: [Gupta - Embedding Tree Metrics into Low-Dimensional Euclidean Spaces (2000); Theorem 4.1](https://link.springer.com/article/10.1007/s004540010020)\n",
    "    If $\\mathcal{X}\\times \\mathcal{Y}$ has a hierarchical structure; in that its metric is induced by a full binary tree; then Distortion \n",
    "    $$\n",
    "    \\tau(\\phi) \\le 4\\,\\pi*L^{1/(F-1)}\\, \\sqrt{\\min\\{F,\\log(L)\\}}\n",
    "    ;\n",
    "    $$ where $L=(k+1)/2$ for $F\\in \\mathbb{N}_+,\\, F>1$.\n",
    "3. Subsets of Euclidean space: see paper and results of Johnson and Lindentrauss as well as Matousek; roughly speaking when $k$ points can be bi-Lipschitz embedding into $\\mathbb{R}^{\\mathcal{O}(\\log(k))}$ with low-distortion.  \n",
    "4. Subspaces (metric) of the real line: $\\tau(\\phi)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258939ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortion_type = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f92dd",
   "metadata": {},
   "source": [
    "### Decide on N vs k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc367e",
   "metadata": {},
   "source": [
    "### The Structure of $\\mathcal{X}$\n",
    "\n",
    "#### Case 1: Packing of the Cube $[0,1]^d$ by $k$ Points\n",
    "\n",
    "$\\mathcal{X}\\subset [0,1]^d$ is a packing of $k$ points.  A simple computation shows that\n",
    "$$\n",
    "\\operatorname{sep}(\\mathcal{X})\n",
    "=\n",
    "    \\frac{\n",
    "        k^{1/d}\n",
    "    }{\n",
    "        2\n",
    "    }\n",
    ".\n",
    "$$\n",
    "\n",
    "#### Case 2: Either we set k manually or we consider the no. Points Needed to Pack a Cube:\n",
    "\n",
    "From [George G. Lorentz, Manfred v. Golitschek, and Yuly Makovoz. Constructive approximation - Proposition 1.3](https://link.springer.com/book/9783642646102) the packing number of $[0,\\operatorname{diam}]^d$ in $\\ell_{\\infty}^d$ by $\\ell_{\\infty}^d$-balls of radius $10^{-p}$ is no more than\n",
    "$$\n",
    "k \\ge \n",
    "\\Big\\lceil\n",
    "2^{-d}\n",
    "        \\,\n",
    "        \\Big(10^{p}*\\operatorname{diam}\\Big)^d\n",
    "\\Big\\rceil\n",
    "$$\n",
    "we use this lower-bound to compute $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7a094",
   "metadata": {},
   "source": [
    "### Worst-Case Lipschitz Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf400678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cube or no Cube?\n",
    "cube_k = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d15963",
   "metadata": {},
   "source": [
    "We will use a worst-case Lipschitz constant; so large that it counts all functions from $\\mathbb{R}^d_{p,1}\\cap [0,1]^d$ to $\\{0,1\\}$.  For a fair comparison with the Occam's razor bound.\n",
    "\n",
    "In the case of a grid in $[0,1]^d$ with spacing $10^{-p}$ we have:\n",
    "$$\n",
    "    \\operatorname{Lip}(f)\n",
    "= \n",
    "    \\sup_{x,z\\in \\mathcal{X};\\,x\\neq x}\\, \n",
    "    \\frac{\n",
    "        d_{\\mathcal{Y}}(f(x),f(z))\n",
    "    }{\n",
    "        d_{\\mathcal{X}}(x,z)\n",
    "    }\n",
    "\\le \n",
    "    \\frac{\n",
    "    \\operatorname{diam(\\mathcal{Y})}\n",
    "    }{\n",
    "    \\operatorname{sep}(\\mathcal{X})\n",
    "    }\n",
    "=\n",
    "    \\frac{\n",
    "    1\n",
    "%     \\operatorname{diam}\\sqrt{d}\n",
    "    }{10^{-p}}\n",
    "=\n",
    "%     \\operatorname{diam}\\sqrt{d}\\,\n",
    "    10^{p}\n",
    "$$\n",
    "In the case where $\\mathcal{X}\\subset [0,1]^d$ consists of $k$ maximally spaced (packing) points in $[0,1]^d$ we compute\n",
    "$\n",
    "\\operatorname{sep}(\\mathcal{X}) = \\frac1{\n",
    "2\\, k^{1/d}\n",
    "}\n",
    "$.  Therefore, the maximal Lipschitz constant of any binary function on $\\mathcal{X}$ is at-most\n",
    "$$\n",
    "    \\operatorname{Lip}(f)\n",
    "= \n",
    "    \\sup_{x,z\\in \\mathcal{X};\\,x\\neq x}\\, \n",
    "    \\frac{\n",
    "        d_{\\mathcal{Y}}(f(x),f(z))\n",
    "    }{\n",
    "        d_{\\mathcal{X}}(x,z)\n",
    "    }\n",
    "\\le \n",
    "    \\frac{\n",
    "    \\operatorname{diam(\\mathcal{X})}\n",
    "    }{\n",
    "    \\operatorname{sep}(\\mathcal{X})\n",
    "    }\n",
    "=\n",
    "    \\frac{\n",
    "    1\n",
    "%         \\operatorname{diam}\\sqrt{d}\n",
    "    }{\n",
    "    \\operatorname{sep}(\\mathcal{X})\n",
    "    }\n",
    "\\le\n",
    "    \\frac{\n",
    "    1\n",
    "%         \\sqrt{d}\n",
    "    }{\n",
    "    (2\n",
    "    k^{1/d})^{-1}\n",
    "    }\n",
    "=\n",
    "    2\n",
    "%     \\,\n",
    "%     \\sqrt{d}\n",
    "    \\,\n",
    "    k^{1/d}\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f35c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypical Space: [0,1]^10000000000\n",
      "Distance Between Points on the Grid: 1e-10\n",
      "We Consider The Worst-Case Lipschitz Constant: 2.0000000023025852\n",
      "Number of points:1.00e+05\n"
     ]
    }
   ],
   "source": [
    "# If Manual \n",
    "k_manual = 1*(10**5)\n",
    "\n",
    "# If Cube\n",
    "diam = 1*(10**0)\n",
    "d = 1*(10**3); print('Prototypical Space: [0,'+str(diam)+']^'+str(d))\n",
    "p = 1*(10**1); print('Distance Between Points on the Grid: '+str(10**-p))\n",
    "\n",
    "%run ./SupportFiles/Compute_N_points.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaca908",
   "metadata": {},
   "source": [
    "## Import Packages and Set Global Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60a845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependancies and Packages Loaded\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "%run ./SupportFiles/Dependancies.ipynb\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0c986",
   "metadata": {},
   "source": [
    "# Set Global Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf122326",
   "metadata": {},
   "source": [
    "## Are we Computing Risk Bounds or Concentration Bounds (in $\\mathcal{W}_1$)?\n",
    "- For Risk Bounds set: True\n",
    "- For Concentration Bounds set: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7aaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk_or_Concentration = True\n",
    "Isometric_Embedding = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec9aa2",
   "metadata": {},
   "source": [
    "#### Decide on How Many Points to Use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c8a6b",
   "metadata": {},
   "source": [
    "Load Global hyperparameters, now that internal parameters have been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80457878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are to Hold with Probability at-least: 95.0%\n",
      "Max Representation Space Dimension (F aka m): 4.605170185988092\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "%run ./SupportFiles/Global_Hyperparameters.ipynb\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab4437",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec195e",
   "metadata": {},
   "source": [
    "#### Load helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "print('---')\n",
    "%run ./SupportFiles/Helper_Functions.ipynb\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06efb2",
   "metadata": {},
   "source": [
    "# The Risk Bound Computers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fa939",
   "metadata": {},
   "source": [
    "We compare our risk bound against the following binary classification benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc565b3",
   "metadata": {},
   "source": [
    "## Benchmark: Trivial VC/Occam Bound (Binary Classification)\n",
    "From the The (Quantitative) Fundamental Theorem of Statistical Learning ([Theorem 6.8 in \"Understanding Machine Learning\" (with extra details in Section 28.1)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=kezPqwoAAAAJ&citation_for_view=kezPqwoAAAAJ:XiVPGOgt02cC)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\sup_{f \\in \\{0,1\\}^{\\mathcal{X}}}\n",
    "    \\,\n",
    "    |\\mathcal{R}(f) - \\hat{\\mathcal{R}}(f)|\n",
    "\\le &\n",
    "    n^{-1/2}\n",
    "    (\n",
    "        128 \\operatorname{VC-dim}\\log(n+1) + \\log(8/\\delta)\n",
    "    )^{1/2}\n",
    "\\\\ \n",
    "\\le & \n",
    "    n^{-1/2}\n",
    "    (\n",
    "        128 \\#\\mathcal{X}\\log(n+1) + \\log(8/\\delta)\n",
    "    )^{1/2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where we use the fact that $\\operatorname{VC-dim}(2^{\\mathcal{X}})\\le \\log_2(\\#2^{\\mathcal{X}})=\\#\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a46477",
   "metadata": {},
   "source": [
    "## Agnostic PAC Bound\n",
    "\n",
    "\n",
    "From [Corollary 4.6 of \"Understanding Machine Learning\"](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=kezPqwoAAAAJ&citation_for_view=kezPqwoAAAAJ:XiVPGOgt02cC) we have the following agnostic PAC learnability bound\n",
    "$$\n",
    "    \\sup_{f \\in \\{0,1\\}^{\\mathcal{X}}}\n",
    "    \\,\n",
    "    |\\mathcal{R}(f) - \\hat{\\mathcal{R}}(f)|\n",
    "\\le \n",
    "\\begin{aligned}\n",
    "    (2n)^{-1/2}\n",
    "    \\log\\big(\n",
    "    2 \\times 2^k/\\delta\n",
    "    \\big)^{1/2}\n",
    "    = \n",
    "    (2n)^{-1/2}\n",
    "     \\big(\n",
    "     \\log(2/\\delta) + k\\log(2)\n",
    "     \\big)^{1/2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2e371",
   "metadata": {},
   "source": [
    "## Generate Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398408fe",
   "metadata": {},
   "source": [
    "#### Decide on Scale of Sample Size (Range: Small N or Large N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab18108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "Freq_plot = 10**2\n",
    "if N_is_Small:\n",
    "    indexing_set = range(1,10**5,Freq_plot)\n",
    "else:\n",
    "#     indexing_set = range(3,10000,Freq_plot)\n",
    "    indexing_set = range(1,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d340cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_Occam = True\n",
    "Agnostic_PAC_Bound = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd5ae8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd0702",
   "metadata": {},
   "source": [
    "Load risk bound functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./SupportFiles/Risk_Bound_Engine.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdefe5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fed9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.title(''+dist_name+', k='+str('{:.2e}'.format(k))+', Varying F and N')\n",
    "    \n",
    "\n",
    "plt.xlabel('No. Samples (N)')\n",
    "plt.ylabel('Risk Bound')# on $\\sup_{f\\in \\mathcal{F}}\\,|\\mathcal{R}(\\hat{f})-\\hat{\\mathcal{R}}(\\hat{f})|$')\n",
    "# plt.ylabel('Bound on $\\mathbb{E}[\\mathcal{W}_1(\\mathbb{P},\\mathbb{P}^N)]$')\n",
    "\n",
    "\n",
    "\n",
    "# Set Max No Colours\n",
    "n_col=max(indexing_set)+2\n",
    "\n",
    "\n",
    "#### Plot Generation\n",
    "for i in indexing_set:\n",
    "    # Set Value of F\n",
    "    F_global = int(np.round(i,0))\n",
    "    \n",
    "    # Write Function of N only\n",
    "    def get_risk_bound__NVar_scalar(N):\n",
    "        return get_risk_bound__scalar(k=k_global,F=F_global,N=N)\n",
    "    \n",
    "    get_risk_bound__NVar = np.vectorize(get_risk_bound__NVar_scalar)\n",
    "    \n",
    "    # Compute Risk Bound\n",
    "    Risk_Bounds = get_risk_bound__NVar_scalar(N_sequence)\n",
    "\n",
    "    \n",
    "    # Plot\n",
    "    col_index = (i-min(indexing_set))/(max(indexing_set)+1)\n",
    "    # Plot Risk-Bound\n",
    "    plot_ratio =int(max(indexing_set)/max(1,Freq_plot*10))  # plot small subset of rep. dims.\n",
    "    if N_is_Small:\n",
    "        # Ony write every other legend\n",
    "        if i % plot_ratio == 1: \n",
    "            plt.plot(N_sequence, \n",
    "                 Risk_Bounds,\n",
    "                 color=colorFader(c1,c2,col_index),\n",
    "                 label = 'F='+str(F_global).format(i=i))\n",
    "        else: # Remove legend on most plots\n",
    "            plt.plot(N_sequence, \n",
    "                 Risk_Bounds,\n",
    "                 color=colorFader(c1,c2,col_index),label='_nolegend_')\n",
    "    else:\n",
    "        #plot most of it for large N\n",
    "        if i % 10 == 1: \n",
    "            plt.plot(N_sequence, \n",
    "                 Risk_Bounds,\n",
    "                 color=colorFader(c1,c2,col_index),\n",
    "                 label = 'F='+str(F_global).format(i=i))\n",
    "        else: # Remove legend on most plots\n",
    "            plt.plot(N_sequence, \n",
    "                 Risk_Bounds,\n",
    "                 color=colorFader(c1,c2,col_index),label='_nolegend_')\n",
    "\n",
    "\n",
    "### Classical Risk-Bounds for Binary Classification\n",
    "if Y_is_bin_class == True:\n",
    "    if Print_Occam:\n",
    "        ###----------------###\n",
    "        ### VC/Occam Bound ###\n",
    "        ###----------------###\n",
    "        # Write Function of N only\n",
    "        def get_risk_bound__VC_scalar(N):\n",
    "            return trivial_VCBound(N_in=N,k_in=k,delta = delta)\n",
    "        get_risk_bound__VC = np.vectorize(get_risk_bound__VC_scalar)\n",
    "\n",
    "        # Compute Risk Bound\n",
    "        Risk_Bounds__VC = get_risk_bound__VC(N_sequence)\n",
    "\n",
    "        # Plot Risk-Bound\n",
    "        plt.plot(N_sequence,Risk_Bounds__VC,label = 'Occam',c='orange',linewidth=3)\n",
    "    ###--------------------###\n",
    "    ### Agnostic PAC Bound ###\n",
    "    ###--------------------###\n",
    "    if Agnostic_PAC_Bound:\n",
    "        # Write Function of N only\n",
    "        def get_risk_bound__Agnostic_PAC_Bound_scalar(N):\n",
    "            return Agnostic_PAC_Bound(N_in=N,k_in=k,delta = delta)\n",
    "        get_risk_bound__Agnostic_PAC_Bound = np.vectorize(get_risk_bound__Agnostic_PAC_Bound_scalar)\n",
    "\n",
    "        # Compute Risk Bound\n",
    "        Risk_Bounds__Agnostic_PAC_Bound = get_risk_bound__Agnostic_PAC_Bound(N_sequence)\n",
    "\n",
    "        # Plot Risk-Bound\n",
    "        plt.plot(N_sequence,Risk_Bounds__Agnostic_PAC_Bound,label = 'Agnostic PAC Bound',c='red',linewidth=3)\n",
    "\n",
    "# plt.legend(loc=0) #'best'\n",
    "plt.legend(loc=1) #'Upper right'\n",
    "# plt.legend(loc=2) # 'Upper left'\n",
    "\n",
    "\n",
    "# ------------------------------------------ #\n",
    "# Our Best Risk-Bound  w. Agnostic PAC Bound #\n",
    "# ------------------------------------------ #\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "axes1 = ax.add_axes([0.2, 0.2, 0.3, 0.3]) # inset axes (location first, size second)\n",
    "# if N_is_Small:\n",
    "#     axes1.set_title('High Dim. Rep.',fontsize=15)\n",
    "# else:\n",
    "#     axes1.set_title('Low Dim. Rep.',fontsize=15)\n",
    "\n",
    "\n",
    "# Determined Reduced Indexing Set (Depending if bottom rep. spaces or top are better; i.e. if N is large or small)\n",
    "if N_is_Small:\n",
    "    # top 10 %\n",
    "    indexing_set__reduced = indexing_set[-int(len(indexing_set)*.75):] # top 10%\n",
    "else:\n",
    "    # bottom 10 %\n",
    "    if distortion_type > 2: # 1d reps not worth it in Euclidean case, so we use the 3d min\n",
    "        indexing_set__reduced = indexing_set[:int(len(indexing_set)*.5)] # bottom 10%\n",
    "        indexing_set__reduced = range(1,10,1)\n",
    "    else:\n",
    "        indexing_set__reduced = range(max(3,min(indexing_set)),max(indexing_set),Freq_plot)\n",
    "        indexing_set__reduced = indexing_set__reduced[:int(len(indexing_set)*.5)] # bottom 10%\n",
    "\n",
    "#### Plot Generation\n",
    "for i in range(1,5):#in indexing_set__reduced:\n",
    "    # Set Value of F\n",
    "    F_global = int(np.round(i,0))\n",
    "    \n",
    "    # Write Function of N only\n",
    "    def get_risk_bound__NVar_scalar(N):\n",
    "        return get_risk_bound__scalar(k=k_global,F=F_global,N=N)\n",
    "    \n",
    "    get_risk_bound__NVar = np.vectorize(get_risk_bound__NVar_scalar)\n",
    "    \n",
    "    # Compute Risk Bound\n",
    "    Risk_Bounds = get_risk_bound__NVar_scalar(N_sequence)\n",
    "\n",
    "    \n",
    "    # Plot\n",
    "    col_index = (i-min(indexing_set))/(max(indexing_set)+1)\n",
    "    plt.plot(N_sequence, Risk_Bounds,label = 'F='+str(F_global).format(i=i),color=colorFader(c1,c2,col_index))\n",
    "    \n",
    "#     plt.legend(loc=1) #'Upper right'\n",
    "\n",
    "if Y_is_bin_class == True:\n",
    "#     if Print_Occam:\n",
    "#         ###----------------###\n",
    "#         ### VC/Occam Bound ###\n",
    "#         ###----------------###\n",
    "#         # Write Function of N only\n",
    "#         def get_risk_bound__VC_scalar(N):\n",
    "#             return trivial_VCBound(N_in=N,k_in=k,delta = delta)\n",
    "#         get_risk_bound__VC = np.vectorize(get_risk_bound__VC_scalar)\n",
    "\n",
    "#         # Compute Risk Bound\n",
    "#         Risk_Bounds__VC = get_risk_bound__VC(N_sequence)\n",
    "\n",
    "#         # Plot Risk-Bound\n",
    "#         plt.plot(N_sequence,Risk_Bounds__VC,label = 'Occam',c='orange',linewidth=3)\n",
    "    if Agnostic_PAC_Bound:\n",
    "        ###--------------------###\n",
    "        ### Agnostic PAC Bound ###\n",
    "        ###--------------------###\n",
    "        # Write Function of N only\n",
    "        def get_risk_bound__Agnostic_PAC_Bound_scalar(N):\n",
    "            return Agnostic_PAC_Bound(N_in=N,k_in=k,delta = delta)\n",
    "        get_risk_bound__Agnostic_PAC_Bound = np.vectorize(get_risk_bound__Agnostic_PAC_Bound_scalar)\n",
    "\n",
    "        # Compute Risk Bound\n",
    "        Risk_Bounds__Agnostic_PAC_Bound = get_risk_bound__Agnostic_PAC_Bound(N_sequence)\n",
    "\n",
    "        # Plot Risk-Bound\n",
    "        plt.plot(N_sequence,Risk_Bounds__Agnostic_PAC_Bound,label = 'Agnostic PAC Bound',c='red',linewidth=3)\n",
    "\n",
    "# ---------------------------------------- #\n",
    "# High Representation Space Dimension Only #\n",
    "# ---------------------------------------- #\n",
    "# if N_is_Small:\n",
    "sns.set_style(\"ticks\")\n",
    "axes2 = ax.add_axes([0.44, 0.55, 0.3, 0.3]) # inset axes (location first, size second)\n",
    "if N_is_Small:\n",
    "    axes2.set_title('Large Rep. Dims.',fontsize=15)\n",
    "else:\n",
    "    axes2.set_title('Low Rep. Dims.',fontsize=15)\n",
    "\n",
    "#### Plot Generation\n",
    "for i in indexing_set__reduced:\n",
    "    # Set Value of F\n",
    "    F_global = int(np.round(i,0))\n",
    "\n",
    "    # Write Function of N only\n",
    "    def get_risk_bound__NVar_scalar(N):\n",
    "        return get_risk_bound__scalar(k=k_global,F=F_global,N=N)\n",
    "\n",
    "    get_risk_bound__NVar = np.vectorize(get_risk_bound__NVar_scalar)\n",
    "\n",
    "    # Compute Risk Bound\n",
    "    Risk_Bounds = get_risk_bound__NVar_scalar(N_sequence)\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    col_index = (i-min(indexing_set))/(max(indexing_set)+1)\n",
    "    plt.plot(N_sequence, Risk_Bounds,label = 'F='+str(F_global).format(i=i),color=colorFader(c1,c2,col_index))\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xlabel('No. Samples (N)',fontsize=10)\n",
    "plt.ylabel('Risk Bound',fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# ----------------------------------------------------- #\n",
    "\n",
    "plt.savefig('Experiments/Risk_Bound_F/Vs_VCBound__k_'+str(N_is_Small)+'____'+str(dist_name__plot)+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e96a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('N Min: '+latex_float(N_min))\n",
    "print('N Max: '+latex_float(N_max))\n",
    "print('For Paper:')\n",
    "print(latex_float(N_min)+'<N<'+latex_float(N_max))\n",
    "print('dimension: '+latex_float(d))\n",
    "print('N. Points (k): '+latex_float(k_manual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe786fcd",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin #\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede43f1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66186ca",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
